# cep146-project1
This is our group's repository for project-software-development in cep146.
Our topic is about **the ethics of AI in software development**.

Rubric and project outline: https://github.com/seneca-cep146/cep146/blob/main/project/project-software-development.md

---

## Group members:
- Victer Yoo
- Minh Tuan Ho
- Cozmo Walker



It’s 2025 AI has become a huge part of software development. Tools like ChatGPT, Copilot and DeepSeek can generate code and suggesting fixes on code resulting in boost in productivity, while AI just seems like a great tool that is always here to help us it also raises important ethical concerns including bias data, privacy copyright and accountability. 

 
[](https://apnews.com/article/business-apple-inc-artificial-intelligence-00c1dab0a727456df9e5ef9c6160c792)

Google’s AI ethics controversy 

Timnit Gebru, an Ai ethics researcher at Google, was forced out of the company after disagreement about a research paper about ethical risks in large language models. 

Her paper discussed  

the bias and fairness of data in AI models 

transparency and accountability in AI models 


Bias data  

AI models learn from existing data, which may include biased patterns or language. This can lead to unfair code suggestions or reinforcement of stereotypes. 

Timnit Gebru’s latest paper warned large language model own system might reinforce stereotypes or misinformation. 


Privacy 

Large language models are trained on data from internet these can sometimes include private or copy righted materials. 

Training data for AI models can also include sensitive or personal / information without notice 


Copyright 

AI systems often use online text, code, or art without explicit permission. 

 

Accountability and Transparency 

When AI systems cause harm (e.g misinformation inefficient or outdated generated code) who is responsible? 

It’s unclear who is responsible when AI-generated code causes bugs, security flaws, or ethical issues. 

 

Final questions 
If we use AI tools and it generates codes that causes harm who should be responsible? The developers? The company who built the ai? 
